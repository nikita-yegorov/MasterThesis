# -*- coding: utf-8 -*-
"""02_ver2_10e_DcGAN_cifar_tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UeHdncqEkbu6Um6ZZx39UubiJFE8SlGg
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
import matplotlib.pyplot as plt
import torchvision
from torchvision import transforms
import torch.nn as nn
from PIL import Image 
from torchvision.utils import save_image
import os
import wandb
import argparse
import data_loader
from tqdm import tqdm
from skimage import feature as feat
import statistics as s

wandb.init(project="Discriminator", entity="nikita_yegorov")


# %matplotlib inline
plt.style.use("ggplot")


class CFG:
  batch_size = 64
  num_epochs = 10
  workers = 4
  seed = 2021
  image_size = 64
  download = True
  dataroot = "data"
  nc = 3 ## number chanels
  ngf = 64 #Size of feature maps in generator
  nz = 100 #latent random input vector
  ndf = 224 #Size of feature maps in discriminator
  lr = 0.0002
  device = 'cuda:1'
  sample_dir = "./images/"

if not os.path.exists(CFG.sample_dir):
  os.makedirs(CFG.sample_dir)


parser = argparse.ArgumentParser(description='Finetune')
parser.add_argument('--model', type=str, default='resnet')
parser.add_argument('--batchsize', type=int, default=64)
# parser.add_argument('--src', type=str, default='amazon')
# parser.add_argument('--tar', type=str, default='webcam')
# parser.add_argument('--data', type=str, default='/mnt/tank/scratch/negorov/ROS/data/office')

parser.add_argument('--src', type=str, default='Real World')
parser.add_argument('--tar', type=str, default='Clipart')
parser.add_argument('--data', type=str, default='/mnt/tank/scratch/negorov/Datasets/OfficeHomeDataset')

parser.add_argument('--n_class', type=int, default=31)
parser.add_argument('--lr', type=float, default=1e-5)
parser.add_argument('--n_epoch', type=int, default=100)
parser.add_argument('--momentum', type=float, default=0.9)
parser.add_argument('--decay', type=float, default=5e-4)
# parser.add_argument('--data', type=str, default='/mnt/tank/scratch/negorov/ROS/data/office')
parser.add_argument('--early_stop', type=int, default=20)
args = parser.parse_args()

# Parameter setting
DEVICE = torch.device('cuda:1')
BATCH_SIZE = {'src': int(args.batchsize), 'tar': int(args.batchsize)}  


print(CFG.device)

import sys
import datetime
# orig_stdout = sys.stdout
# now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')

# directory = 'images/' + now
# if not os.path.exists(directory):
#     os.makedirs(directory)


# f = open('out.txt--' + now, 'w')
# sys.stdout = f

# unnormalization image from range (-1)-0 to range 0-1 to display it
def unnomalization(x):
    x = (x - x.min()) / (x.max() - x.min())
    return x

# Create the dataloader


"""
## GAN 
состоит из двух глубоких сетей, генератора и дискриминатора. генератор создает изображения, прежде чем научиться его обучать. Поскольку дискриминатор представляет собой модель бинарной классификации, мы можем использовать функцию потери бинарной кросс-энтропии для количественной оценки того, насколько хорошо он может различать реальные и сгенерированные изображения.
"""

import torch.nn as nn
import torch.nn.functional as F
class Generator(nn.Module):
    def __init__(self, nc, nz, ngf):
        #ConvTranspose2d - BatchNorm - Relu -ConvTranspose2d - BatchNorm - Relu -ConvTranspose2d - BatchNorm - Relu 
        #ConvTranspose2d - BatchNorm - Relu - ConvTranspose2d - Tanh
        super(Generator, self).__init__()
        self.h = nn.Sequential(
            nn.ConvTranspose2d(CFG.nz, CFG.ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(CFG.ngf * 8),
            nn.ReLU(True),

            nn.ConvTranspose2d(CFG.ngf * 8, CFG.ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(CFG.ngf * 4),
            nn.ReLU(True),

            nn.ConvTranspose2d(CFG.ngf * 4, CFG.ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(CFG.ngf * 2),
            nn.ReLU(True),

            nn.ConvTranspose2d(CFG.ngf * 2, CFG.ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(CFG.ngf),
            nn.ReLU(True),

            nn.ConvTranspose2d(CFG.ngf, CFG.nc, 4, 2, 1, bias=False)
        )


    def forward(self, x):
        x = self.h(x)
        return torch.tanh(x)


class Discriminator(nn.Module):
    def __init__(self, nc, ndf):
    #conv2d - leaky - conv2d - batchnorm - leaky - conv2d - batchnorm - leaky - conv - batchnorm - leaky - conv2d
        super(Discriminator, self).__init__()

        self.h = nn.Sequential(
            nn.Conv2d(CFG.nc, CFG.ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(CFG.ndf, CFG.ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(CFG.ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(CFG.ndf * 2, CFG.ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(CFG.ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(CFG.ndf * 4, CFG.ndf * 8, 4, 3, 1, bias=False),
            nn.BatchNorm2d(CFG.ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(CFG.ndf * 8, 1, 4, 6, 0, bias=False)
        )


    def forward(self, x):
        x = self.h(x)   
        return torch.sigmoid(x)

# create new Generator model
# G = Generator(CFG.nc, CFG.nz, CFG.ngf)
# create new Discriminator model
D = Discriminator(CFG.nc, CFG.ndf)


def canny_images(imags, save=False, domain = ' S ', tr_val=' tr '):
    c_im = imags.detach().numpy()
    c_im = np.transpose(c_im,(0,2,3,1))
    c_im = np.mean(c_im, 3)
    # edges1 = np.array([feat.canny(i) for i in c_im])
    thresh = []
    sigma = 0.33

    for i in range(c_im.shape[0]):
        v = np.median(c_im[i])

        lower_thresh_i = float(max(0, (1.0 - sigma) * v))
        upper_thresh_i = float(min(1, (1.0 + sigma) * v))

        thresh.append([lower_thresh_i, upper_thresh_i])

    edges1 = np.array([feat.canny(c_im[i],
                        low_threshold=thresh[i][0],
                        high_threshold=thresh[i][1],
                        sigma=sigma
                        ) for i in range(c_im.shape[0])])

    
    if save:
        now1 = datetime.datetime.now().strftime('%b%d_%H-%M-%S')
        img = edges1[0]
        #img = np.transpose(edges1[0], (1,2,0))
        plt.imshow(img)
        name = '/Canny_im '+domain+tr_val +now1 
        # plt.savefig(directory + name)
    edges1 = np.stack((edges1,)*3, axis=1)
    im_canny = torch.from_numpy(edges1)
    im_canny = im_canny.type(torch.float)
    return im_canny



print(D)

# show the output of model 

# x = D(y)
# print( y.shape, x.shape)

# define the criterion is nn.BCELoss()
criterion = nn.BCELoss()
## Define the optimizer for generator and discrimator
d_optim = torch.optim.Adam(D.parameters(),lr=CFG.lr)


def reset_grad():
    ## reset gradient for optimizer of generator and discrimator
    d_optim.zero_grad()


def train_discriminator(source_images, target_images, canny=False):

    # Reset gradients
    reset_grad()
  
    # Create the labels which are later used as input for the BCE loss
    source_labels = torch.ones(CFG.batch_size, 1, 1, 1).to(CFG.device)
    target_labels = torch.zeros(CFG.batch_size, 1, 1, 1).to(CFG.device)
        
    outputs = D(source_images)


    # Loss for real images
    # print(outputs.shape, source_labels.shape)
    # exit()
    loss_source = criterion(outputs, source_labels)
        
    source_score = outputs

    # Loss for fake images

    outputs = D(target_images)
    loss_target = criterion(outputs, target_labels)

    target_score = outputs

    # Sum losses
    d_loss = loss_source + loss_target

    # Compute gradients
    if not canny:
        loss_source.backward()
        loss_target.backward()

        # Adjust the parameters using backprop
        d_optim.step()
    return d_loss, source_score, target_score



"""## Start the training proccess"""



d_losses,target_scores, fake_scores = [], [], [] 
root_dir = args.data
domain = {'src': str(args.src), 'tar': str(args.tar)}
dataloaders = {}
dataloaders['tar_tr'], dataloaders['tar_val'] = data_loader.load_train(root_dir, domain['tar'], BATCH_SIZE['tar'], 'tar')
dataloaders['src_tr'], dataloaders['src_val'] = data_loader.load_train(root_dir, domain['src'], BATCH_SIZE['src'], 'src')



val_step = 0
for epoch in range(1, args.n_epoch + 1):
    # iters = tqdm(zip(dataloaders['src_tr'], dataloaders['tar_tr']), desc=f'epoch {epoch} ', total=
    #                                                 min(len(dataloaders['src_tr']), len(dataloaders['tar_tr'])))
    iters = zip(dataloaders['src_tr'], dataloaders['tar_tr'])
    iters_val = zip(dataloaders['src_val'], dataloaders['tar_val'])
    D.to(DEVICE)
    for i, ((im_source, label_source), (im_target, label_target)) in enumerate(iters):
        print('train', i, im_source.shape, im_target.shape)
        s_b, t_b = im_source.shape[0], im_target.shape[0]
        if s_b != CFG.batch_size or t_b != CFG.batch_size:
            continue
        
        im_source_c_tar = canny_images(im_source, save=False, domain = ' S ')        
        im_target_c_tar = canny_images(im_target, save=False, domain = ' T ') 



        im_source_ = im_source_c_tar.to(DEVICE)
        im_target_ = im_target_c_tar.to(DEVICE)

        # Train the discriminator  
        d_loss, source_score, target_score = train_discriminator(im_source_, im_target_)

        print('Epoch [{}/{}], Step [{}/{}], d_loss_c: {:.4f},  D(source_c): {:.2f}, D(target_c): {:.2f}' 
                  .format(epoch, args.n_epoch, i+1, iters, d_loss.item(),  
                          source_score.mean().item(), target_score.mean().item()))
        wandb.log({"d_loss_train_c": d_loss.item(),
                    "source_score_train_c": source_score.mean().item(),
                    "target_score_train_c": target_score.mean().item()
                    })
        if i % 2 == 0 and i > 0:
            d_losses,source_sc, target_sc = [], [], []
            with torch.no_grad():
                # D.eval()
                val_step +=1
                
                for i_val, ((im_source_val, label_source), (im_target_val, label_target)) in enumerate(iters_val):
                    print('i_val', i_val, im_source_val.shape, im_target_val.shape)
                    s_b, t_b = im_source_val.shape[0], im_target_val.shape[0]
                    if s_b !=CFG.batch_size or t_b != CFG.batch_size:
                        print('not equal batches')
                        continue

                    im_source_c = canny_images(im_source_val, save=False, domain = ' S ', tr_val=' val ')        
                    im_target_c = canny_images(im_target_val, save=False, domain = ' T ', tr_val=' val ')        


                    im_source_val = im_source_c.to(DEVICE)
                    im_target_val = im_target_c.to(DEVICE)

                    d_loss, source_score, target_score = train_discriminator(im_source_val, im_target_val, True)
                    # print('d_loss, source_score, target_score', d_loss.item(), 
                    #             source_score.mean().item(), target_score.mean().item())
                
                    if i_val == 12:
                        print('WANDB LOG')
                        wandb.log({"d_loss_val_c": s.mean(d_losses),
                            "source_score_val_c": s.mean(source_sc),
                            "target_score_val_c": s.mean(target_sc),
                            "val_step_c": val_step
                            })
                    d_losses.append(d_loss.item())
                    source_sc.append(source_score.mean().item())
                    target_sc.append(target_score.mean().item())
                    print(len(d_losses), len(source_sc),len(target_sc),)
                    print(s.mean(d_losses), s.mean(source_sc),s.mean(target_sc),)
            print('Val LEN d_loss',len(d_losses))
            if len(d_losses)!=0:
                print('Epoch [{}/{}], Step [{}/{}], d_loss_val_c: {:.4f},  D(source_val_c): {:.2f}, D(target_val_c): {:.2f}' 
                .format(epoch, args.n_epoch, i+1, iters, s.mean(d_losses),  
                        s.mean(source_sc),  s.mean(target_sc)))
                # exit()
                # wandb.log({"d_loss_val_c": s.mean(d_losses),
                #     "source_score_val_c": s.mean(source_sc),
                #     "target_score_val_c": s.mean(target_sc),
                #     "val_step_c": val_step
                #     })

                # D.train()
                          
                    


                

        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        # print('Epoch [{}/{}], d_loss: {:.4f},  D(x): {:.2f}' 
        #           .format(epoch, args.n_epoch,  d_loss.item(),  
        #                  ))
        # Inspect the losses
        # for phase in ['src', 'val', 'tar']:
        #     if phase == 'src':
        #         D.train()
        #     else:
        #         D.eval()
        #     total_loss, correct = 0, 0
        #     #for ((im_source, label_source), (im_target, label_target)) in zip(dataloaders[phase]:
        #         inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

        #         d_optim.zero_grad()
        #         with torch.set_grad_enabled(phase == 'src'):
        #             outputs = D(inputs)
        #             loss = criterion(outputs, labels)
        #         preds = torch.max(outputs, 1)[1]
        #         if phase == 'src':
        #             loss.backward()
        #             d_optim.step()
        #         total_loss += loss.item() * inputs.size(0)
        #         correct += torch.sum(preds == labels.data)
        #     epoch_loss = total_loss / len(dataloaders[phase].dataset)
        #     epoch_acc = correct.double() / len(dataloaders[phase].dataset)
            
# sys.stdout = orig_stdout
# f.close()

